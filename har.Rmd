---
title: "Human Activity Recognition"
author: "Lakin Ducker"
date: "Thursday, June 19, 2014"
output: html_document
---

## Load Libraries and Read in Data

```{r}
# necessary libraries
library(caret)

# set working directory
setwd("D:/R_MachineLearning/Project")

# load data
trainRawData <- read.csv("pml-training.csv",na.strings=c("NA",""))
```


## Exploratory Data Analysis 

I conducted exploratory data analysis on the raw training data in order to determine which predictors may have high predictive value and which predictors should be excluded. I made a plot for each variable with the classe variable to look for relationships. It became quickly clear that the variables that mostly had NA values were not valuable predictors. 

```{r}
plot(trainRawData$kurtosis_picth_arm, trainRawData$classe)
```

Above is an example of one such variable. The code below removed those variables with NAs. 

# Remove NA's

```{r}
# remove NA's
NAs <- apply(trainRawData,2,function(x) {sum(is.na(x))}) 
goodData <- trainRawData[,which(NAs == 0)]
```

## Create Training and Testing Data Subsets

I then created training and testing data subsets, which I named trainData and testData. I needed to use only 30% of the data for the trainData subset because of insufficient RAM on my laptop. However, this appears to have yielded a  sufficiently high accuracy. I would have preferred to have used 60% of the data for the trainData subset.

```{r}
# make trainData set
trainIndex <- createDataPartition(y = goodData$classe, p=0.3,list=FALSE) 
trainData <- goodData[trainIndex,]

# make a testData set
testData <- goodData[-trainIndex,]
```

# Remove Non-Useful and Problematic Predictors

I also chose to remove some other predictors that I felt either did not provide predictive value or that could be problematic in overfitting. 

```{r}
plot(trainData$X, trainData$classe)
```

Above is an example of one such variable. The variable X is the index. While there is clearly a relationship with classe, there was much evidence that it was just showing a temporal pattern. There was additional evidence that this could lead to overfitting and, thus, poor prediction accuracy. The variable "X" and the variables containing "timestamp", "user_name" and "new_window" were removed.

```{r}
# remove other predictors with little predictive value
removeIndex <- grep("timestamp|X|user_name|new_window",names(trainData))
trainData <- trainData[,-removeIndex]
```

## Build Machine Learning Algorithm

I decided to use Random Forest as the method in my model because of its ability to find a natural balance between high variance or high bias. There also was no clear underlying model that could be seen simply in exploratory data analysis. 

```{r}
# create model
modFit <- train(trainData$classe ~.,data=trainData,method="rf",prox=TRUE)
modFit
```

I then built a predictor with the model.

```{r}
# build predictor
table(predict(modFit,trainData),trainData$classe)
```

## Cross Validation and Out of Sample Error

I did cross-validation on the testData subset. 

```{r}
# try on testData
confusionMatrix(predict(modFit,testData),testData$classe)
```

Above are the results of validating the analysis on the testData subset. The Confusion Matrix above shows the expected Accuracy. The expected Out of Sample Error is simply 1 - Accuracy. 

## Prediction

At this point I applied my machine learning algorithm to the 20 test cases provided for the project.

```{r}
# load testing data
testing <- read.csv("pml-testing.csv", header=TRUE)

# Apply on test
testPred <- predict(modFit,testing) 
testPred
```